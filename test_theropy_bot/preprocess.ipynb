{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the necessary imports\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    A function that cleans the text by removing the common abbreviations and unwanted characters or puntuations\n",
    "    It also ends up adding a <start> tag at the beginning of the text and\n",
    "    and <end> tag at the last of the text\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()   # lowercase and remove trailing whitespaces\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)   # remove extra spaces in between\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = '<start> ' + text + ' <end>'\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset_folder_path, len_bound, num_examples = None):\n",
    "    \"\"\"\n",
    "    It reads the required files, creates questions and answers based on the conversations.\n",
    "    \"\"\"\n",
    "    min_sentence_length = len_bound[0]\n",
    "    max_sentence_length = len_bound[1]\n",
    "    \n",
    "    lines = open(str(dataset_folder_path) + '/dialog.txt',encoding='utf-8', errors = 'ignore').read().split('\\n')\n",
    "    conv_lines = open(str(dataset_folder_path) + '/conv.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "\n",
    "    # Create a dictionary to map each line's id with its text\n",
    "    id2line = {}\n",
    "    sent_len = {}   # create a dictionary to contain sentence lengths\n",
    "    \n",
    "    for line in lines:\n",
    "        _line = line.split(' + ')\n",
    "        if len(_line) == 2:\n",
    "            speech = clean_text(_line[1])\n",
    "            id2line[_line[0]] = speech\n",
    "            sent_len[_line[0]] = len(speech.split(' '))\n",
    "\n",
    "\n",
    "    # Create a list of all of the conversations' lines' ids.\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line[1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "\n",
    "    # Sort the sentences into questions (inputs) and answers (targets)\n",
    "    input_lang = []\n",
    "    output_lang = []\n",
    "    if num_examples is not None:\n",
    "        convs = convs[:num_examples]\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv)-1):\n",
    "            if (len[conv[i]] <= max_sentence_length   and \n",
    "                len[conv[i+1]] <= max_sentence_length and \n",
    "                len[conv[i]] >= min_sentence_length   and \n",
    "                len[conv[i+1]] >= min_sentence_length ):\n",
    "                # we do not use very long sentences\n",
    "                input_lang.append(id2line[conv[i]])\n",
    "                output_lang.append(id2line[conv[i+1]])\n",
    "\n",
    "    assert len(input_lang) == len(output_lang)\n",
    "    print(\"Read %s sentence pairs\" % len(input_lang))\n",
    "        \n",
    "    return (input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang, oov=True):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into words, and correspondingly create an index based representation for vocabulary\n",
    "    \"\"\"\n",
    "    if oov:\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token = '<unk>')\n",
    "    else:\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_folder_path, len_bound, num_examples = None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = preprocess(dataset_folder_path, len_bound, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang, oov = True)   # in the input language, we allow OOV words\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang, oov = False)   # in the output language, we do not allow OOV words\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_folder_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtheropy_conversations\u001b[39m\u001b[39m'\u001b[39m   \u001b[39m# the path to the folder \u001b[39;00m\n\u001b[0;32m      3\u001b[0m len_bounds \u001b[39m=\u001b[39m [\u001b[39m2\u001b[39m, \u001b[39m20\u001b[39m]   \u001b[39m# minimum and maximum permissible length of a sentence to be considered.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m input_tensor, target_tensor, inp_lang, targ_lang \u001b[39m=\u001b[39m load_dataset(dataset_folder_path, len_bounds, num_examples \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m      7\u001b[0m inp_lang_json \u001b[39m=\u001b[39m inp_lang\u001b[39m.\u001b[39mto_json()\n\u001b[0;32m      8\u001b[0m targ_lang_json \u001b[39m=\u001b[39m targ_lang\u001b[39m.\u001b[39mto_json()\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(dataset_folder_path, len_bound, num_examples)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(dataset_folder_path, len_bound, num_examples \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# creating cleaned input, output pairs\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     targ_lang, inp_lang \u001b[39m=\u001b[39m preprocess(dataset_folder_path, len_bound, num_examples)\n\u001b[0;32m      5\u001b[0m     input_tensor, inp_lang_tokenizer \u001b[39m=\u001b[39m tokenize(inp_lang, oov \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)   \u001b[39m# in the input language, we allow OOV words\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     target_tensor, targ_lang_tokenizer \u001b[39m=\u001b[39m tokenize(targ_lang, oov \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)   \u001b[39m# in the output language, we do not allow OOV words\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 37\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(dataset_folder_path, len_bound, num_examples)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m convs:\n\u001b[0;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(conv)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m         \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39;49m[conv[i]] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_sentence_length   \u001b[39mand\u001b[39;00m \n\u001b[0;32m     38\u001b[0m             \u001b[39mlen\u001b[39m[conv[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_sentence_length \u001b[39mand\u001b[39;00m \n\u001b[0;32m     39\u001b[0m             \u001b[39mlen\u001b[39m[conv[i]] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_sentence_length   \u001b[39mand\u001b[39;00m \n\u001b[0;32m     40\u001b[0m             \u001b[39mlen\u001b[39m[conv[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_sentence_length ):\n\u001b[0;32m     41\u001b[0m             \u001b[39m# we do not use very long sentences\u001b[39;00m\n\u001b[0;32m     42\u001b[0m             input_lang\u001b[39m.\u001b[39mappend(id2line[conv[i]])\n\u001b[0;32m     43\u001b[0m             output_lang\u001b[39m.\u001b[39mappend(id2line[conv[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset_folder_path = 'theropy_conversations'   # the path to the folder \n",
    "    len_bounds = [2, 20]   # minimum and maximum permissible length of a sentence to be considered.\n",
    "    \n",
    "    input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(dataset_folder_path, len_bounds, num_examples = None)\n",
    "    \n",
    "    inp_lang_json = inp_lang.to_json()\n",
    "    targ_lang_json = targ_lang.to_json()\n",
    "    \n",
    "    with open('processed_data/inp_lang.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(inp_lang_json, ensure_ascii=False))\n",
    "        f.close()\n",
    "    print('Input Language Tokenizer saved...')\n",
    "        \n",
    "    with open('processed_data/targ_lang.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(targ_lang_json, ensure_ascii=False))\n",
    "        f.close()\n",
    "    print('Target Language Tokenizer saved...')\n",
    "        \n",
    "    np.savez('processed_data/data.npz', input_tensor, target_tensor)\n",
    "    print('Final Dataset saved...')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a7a61ba480da9f5c4bd5a664dbae6700f865d2dc3f2f9836cf81f72aa54481"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
